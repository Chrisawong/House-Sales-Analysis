---
title: "House Sales Data Analysis in King County, WA"
author: "Christopher Wong"
date: "Created: 2023-04-25 Updated: 2025-01-18"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Online property companies use machine learning techniques to provide house valuations. This project aims to predict house sales in King County, Washington State, USA using Multiple Linear Regression (MLR). The dataset includes historical records of houses sold between May 2014 and May 2015. This project was created by Christopher Wong, for Professor You Liang (course MTH404).

## DATA

The data for this project was sourced from the Kaggle dataset titled "KC_Housesales_Data". The dataset can be accessed at: <https://www.kaggle.com/swathiachath/kc-housesales-data>

```{r}
library(tidyverse)
library(corrplot)
library(lubridate)
library(readr)
library(caTools)
library(GGally)
library(caret)
library(leaps)
```

```{r}
house_data <- read_csv("kc_house_data.csv")

# Display the first few rows and structure of the data
head(house_data)
str(house_data)
summary(house_data)
```

The dataset includes 21 independent variables such as bedrooms, sqft_living, view, and grade, with the dependent variable being price. It comprises 21,597 observations.

```{r}
# Check for missing values
na_values <- data.frame(no_of_na_values = colSums(is.na(house_data)))
head(na_values, 21)
```

There are no missing values in this data.

## EXPLORATORY DATA ANALYSIS ON THE TRAINING DATA

Now we modify the data slightly and add two new columns for better understanding. Price might depend on the age of the house and the number of times it has been renovated. Therefore, we extract the age and renovation count of each house from our training data.

```{r}
# Modify the data to add age and renovation status
date_sale <- mdy(house_data$date)
house_data$sale_date_year <- as.integer(year(date_sale))
house_data$age <- house_data$sale_date_year - house_data$yr_built

house_data$reno <- ifelse(house_data$yr_renovated == 0, 0, 1)
house_data$reno <- as.factor(house_data$reno)
```

### Training and test data

We divide the data to be the training set (80%) and test set (20%). If we have already the training and test set provided, then we do not need to add this step.

```{r}
set.seed(500827260)

# Get the number of rows in the dataset
n <- nrow(house_data)

# Calculate the number of test samples (20% of the data)
ntest <- trunc(0.2 * n)

# Randomly sample indices for the test set
testid <- sample(1:n, ntest)

# Split the data into training and test sets
train_data <- house_data[-testid, ]
test_data <- house_data[testid, ]
```

### Check the response variable

The price is skewed to the right with several very high prices.

```{r}
boxplot(train_data$price)
hist(train_data$price)
```

### Determining the association between variables.

We take out the correlation plot (corrplot) to understand the association of the dependent variable (price) with the independent variables.

```{r}
# Extract relevant columns for correlation analysis
cor_data <- data.frame(train_data[, 3:21])
# Calculate/display the correlation matrix
correlation <- cor(cor_data)
correlation
# Setup and plot the correlation matrix
par(mfrow = c(1, 1))
corrplot(correlation, method = "color")
```

Price is strongly positively correlated with bathroom, Sqft_living, grade, sqft_above, sqft_living15. We use scatterplot and boxplot to visualize the relationship between price and some predictors.

```{r}
# Scatter plot of Sqft_living and Price
ggplot(data = train_data, aes(x = sqft_living, y = price)) +
  geom_jitter() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Scatter plot of Sqft_living and Price",
       x = "Sqft_living",
       y = "Price")

# Scatter plot of Sqft_above and Price
ggplot(data = train_data, aes(x = sqft_above, y = price)) +
  geom_jitter() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Scatter plot of Sqft_above and Price",
       x = "Sqft_above",
       y = "Price")

# Scatter plot of Sqft_living15 and Price
ggplot(data = train_data, aes(x = sqft_living15, y = price)) +
  geom_jitter() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Scatter plot of Sqft_living15 and Price",
       x = "Sqft_living15",
       y = "Price")
```

```{r}
# Boxplot of Price by Grade
boxplot(price ~ grade,
        data = train_data,
        main = "Boxplot of Price by Grade",
        xlab = "Grade",
        ylab = "Price",
        col = "orange",
        border = "brown")
```

Check the new added variables:

```{r}
# Scatter plot of Age and Price
ggplot(data = train_data, aes(x = age, y = price)) +
  geom_jitter() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Scatter plot of Age and Price",
       x = "Age",
       y = "Price")

# Boxplot of Price by Renovation Status
boxplot(price ~ reno,
        data = train_data,
        main = "Boxplot of Price by Renovation Status",
        xlab = "Renovation Status",
        ylab = "Price",
        col = "orange",
        border = "brown")
```

### Removing outlier could be optional

We see that we have a significantly large number of outliers.

Treating or altering the outlier/extreme values in genuine observations is not a standard operating procedure. However, it is essential to understand their impact on our predictive models.

To better understand the implications of outliers better, we should compare the fit of a simple linear regression model on the dataset with and without outliers. For this we first extract outliers from the data and then obtain the data without the outliers.

```{r}
# Identify and remove outliers
outliers <- boxplot(train_data$price, plot = FALSE)$out
outliers_data <- train_data[which(train_data$price %in% outliers), ]
train_data1 <- train_data[-which(train_data$price %in% outliers), ]
```

Now plot Now we plot the data with and without outliers.

```{r}
par(mfrow = c(1, 2))

# Plot of original data with outliers
plot(train_data$bedrooms, train_data$price,
     main = "With Outliers",
     xlab = "Bedrooms",
     ylab = "Price",
     pch = "*",
     col = "red",
     cex = 2)
abline(lm(price ~ bedrooms, data = train_data),
       col = "blue",
       lwd = 3,
       lty = 2)

# Plot of data without outliers
plot(train_data1$bedrooms, train_data1$price,
     main = "Outliers Removed",
     xlab = "Bedrooms",
     ylab = "Price",
     pch = "*",
     col = "red",
     cex = 2)
abline(lm(price ~ bedrooms, data = train_data1),
       col = "blue",
       lwd = 3,
       lty = 2)
```

## MODELING

We first use the entire data.

```{r}
# Prepare the training data by removing unnecessary columns
# and converting some columns to factors
train_data.m <- train_data[, -c(1, 2, 15, 16, 17, 22)] %>%
  mutate(waterfront = as.factor(waterfront),
         view = as.factor(view),
         condition = as.factor(condition),
         reno = as.factor(reno))

# Check the structure of the data
str(train_data.m)
# Check the number of columns
ncol(train_data.m) 
```

```{r}
# Fit the model
model.full <- lm (formula = price ~ ., data = train_data.m) 
# Display the model summary
summary(model.full) 
```

```{r}
# Get the model coefficients
models <- regsubsets(price ~ ., data = train_data.m, nvmax = 23)
summary(models)

# Get the best model based on Adjusted R-squared, Cp, and BIC
res.sum <- summary(models)
data.frame(
  Adj.R2 = which.max(res.sum$adjr2),
  CP = which.min(res.sum$cp),
  BIC = which.min(res.sum$bic)
)
```

```{r}
# id: model id
# object: regsubsets object
# data: data used to fit regsubsets
# outcome: outcome variable
get_model_formula <- function(id, object, outcome) {
  # get models data
  models <- summary(object)$which[id, -1]
  # Get model predictors
  predictors <- names(which(models == TRUE))
  predictors <- paste(predictors, collapse = "+")
  # Build model formula
  as.formula(paste0(outcome, "~", predictors))
}

get_model_formula(21, models, "Price")
```

```{r}
# # Fit the linear regression model 1 with selected predictors
model1 <- lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + 
            waterfront + view + condition + grade + sqft_basement + lat + 
            long + sqft_living15 + sqft_lot15 + age + reno, 
            data = train_data.m)
summary(model1)
```

```{r}
# Fit the linear regression model 2 with selected predictors
model2 <- lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors +
             waterfront + view + condition + grade + sqft_basement + lat +
             long + sqft_living15 + sqft_lot15 + age + reno +
             bathrooms * grade + grade * sqft_living15 + grade * sqft_lot15 +
             lat * long,
             data = train_data.m)
summary(model2)
```

### PREDICTION ON THE TEST DATA

```{r}
# Preparing the test data
test_data.m <- test_data[, -c(1, 2, 15, 16, 17, 22)] %>%
  mutate(waterfront = as.factor(waterfront),
         view = as.factor(view),
         condition = as.factor(condition),
         reno = as.factor(reno))
```

```{r}
# Predict the house prices on the test data using model2
pred_test <- predict(newdata = test_data.m, model2)

# Create a data frame to compare actual and predicted prices
tally_table_1 <- data.frame(actual = test_data.m$price, predicted = pred_test)

mean(abs(test_data.m$price - pred_test))
```

## Compare with one-layer forward neural network

```{r}
# Create the model matrix for the predictors and scale the data
x <- model.matrix(price ~ . - 1, data = train_data.m) %>% scale()
y <- train_data.m$price
```

```{=html}
<!-- # ```{r}
# library(keras)

# # Define the neural network model
# modnn <- keras_model_sequential() %>%
#   layer_dense(units = 100, activation = "relu", input_shape = ncol(x)) %>%
#   layer_dropout(rate = 0.2) %>%
#   layer_dense(units = 1)

# modnn %>% compile(
#   loss = "mse", # mean squared error
#   optimizer = optimizer_rmsprop(),
#   metrics = list("mean_absolute_error")
# )
# ```

# ```{r}
# # Prepare the test data
# x.test <- model.matrix(price ~ . - 1, data = test_data.m) %>% scale()
# y.test <- test_data.m$price

# # Train the neural network model
# history <- modnn %>% fit(
#   x.test, y.test, epochs = 1000, batch_size = 32,
#   validation_data = list(x.test, y.test)
# )

# # Predict using the trained model
# npred <- predict(modnn, x.test)

# # Calculate the mean absolute error
# mean(abs(y.test - npred))
# ``` -->
```

The neural network has a similar test error to the multiple linear regression model. However, with additional time, we could tune the parameters to improve the performance of the neural network.
